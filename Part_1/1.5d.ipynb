{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(filepath=\"./Boston-filtered.csv\", testsize=1/3, random_state=None):\n",
    "    \"\"\"\n",
    "    Split a dataset into training and test sets.\n",
    "\n",
    "    This function loads a dataset from a CSV file, shuffles the rows (if a random seed is provided), \n",
    "    and splits the dataset into input features (`X`) and target values (`y`). The dataset is then \n",
    "    divided into training and test sets based on the specified `testsize` ratio.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    filepath : str, optional\n",
    "        The path to the CSV file containing the dataset. The default is \"./Boston-filtered.csv\".\n",
    "    \n",
    "    testsize : float, optional\n",
    "        The proportion of the dataset to include in the test split. Default is 1/3, meaning \n",
    "        that 1/3 of the dataset will be used for testing and the rest for training.\n",
    "    \n",
    "    random_state : int, optional\n",
    "        The seed used by the random number generator for shuffling the data. If `None`, the data \n",
    "        will not be shuffled in a reproducible way. Default is `None`.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    X_train : ndarray\n",
    "        A numpy array containing the training input features.\n",
    "    \n",
    "    y_train : ndarray\n",
    "        A numpy array containing the training target values.\n",
    "    \n",
    "    X_test : ndarray\n",
    "        A numpy array containing the test input features.\n",
    "    \n",
    "    y_test : ndarray\n",
    "        A numpy array containing the test target values.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = pd.read_csv(filepath)\n",
    "    data = data.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    # Separate the features (X) and target values (y)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values \n",
    "\n",
    "    # Calculate the index to split the dataset based on the testsize ratio\n",
    "    split_index = int(len(data) * (1 - testsize))\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train = X[:split_index]\n",
    "    y_train = y[:split_index]\n",
    "\n",
    "    X_test = X[split_index:]\n",
    "    y_test = y[split_index:]\n",
    "\n",
    "    # Return the training and test sets\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_attribute(X_train, y_train, X_test, y_test):\n",
    "    y_mean = np.mean(y_train)\n",
    "\n",
    "    X_train_ones = np.ones(len(X_train))\n",
    "    X_test_ones = np.ones(len(X_test))\n",
    "\n",
    "    y_train_pred = y_mean * X_train_ones\n",
    "    y_test_pred = y_mean * X_test_ones\n",
    "\n",
    "    mse_train = np.mean((y_train - y_train_pred)**2)\n",
    "    mse_test = np.mean((y_test - y_test_pred)**2)\n",
    "\n",
    "    return mse_train, mse_test\n",
    "\n",
    "def single_attribute(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    training_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    for i in range(X_train.shape[1]):\n",
    "        X_train_single = X_train[:, i].reshape(-1, 1)\n",
    "        X_test_single = X_test[:, i].reshape(-1, 1)\n",
    "\n",
    "        X_train_augmented = np.hstack([X_train_single, np.ones((X_train_single.shape[0], 1))])\n",
    "        X_test_augmented = np.hstack([X_test_single, np.ones((X_test_single.shape[0], 1))])\n",
    "\n",
    "        w = np.linalg.inv(X_train_augmented.T @ X_train_augmented) @ X_train_augmented.T @ y_train\n",
    "\n",
    "        y_train_pred = X_train_augmented @ w\n",
    "        y_test_pred = X_test_augmented @ w\n",
    "\n",
    "        mse_train = np.mean((y_train - y_train_pred)**2)\n",
    "        mse_test = np.mean((y_test - y_test_pred)**2)\n",
    "\n",
    "        training_errors.append(mse_train)\n",
    "        test_errors.append(mse_test)    \n",
    "\n",
    "    return training_errors, test_errors\n",
    "\n",
    "def all_attributes(X_train, y_train, X_test, y_test):\n",
    "    X_train_augmented = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_test_augmented = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "\n",
    "    w = np.linalg.inv(X_train_augmented.T @ X_train_augmented) @ X_train_augmented.T @ y_train\n",
    "\n",
    "    y_train_pred = X_train_augmented @ w\n",
    "    y_test_pred = X_test_augmented @ w\n",
    "\n",
    "    mse_train = np.mean((y_train - y_train_pred)**2)\n",
    "    mse_test = np.mean((y_test - y_test_pred)**2)\n",
    "\n",
    "    return mse_train, mse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(xi, xj, sigma):\n",
    "    sq_dists = np.sum(xi**2, axis=1)[:, None] + np.sum(xj**2, axis=1)[None, :] - 2 * np.dot(xi, xj.T)\n",
    "    K = np.exp(-sq_dists / (2 * sigma**2))\n",
    "\n",
    "    return K\n",
    "\n",
    "def kernel_ridge_regression(X_train, y_train, gamma, K):\n",
    "    ell = X_train.shape[0]\n",
    "    I = np.eye(ell)\n",
    "    alpha_star = np.linalg.inv(K + gamma * ell * I) @ y_train\n",
    "\n",
    "    return alpha_star\n",
    "\n",
    "def cross_validation(X_train, y_train, gamma_values, sigma_values):\n",
    "    n = len(y_train)\n",
    "    fold_size = n // 5\n",
    "\n",
    "    best_gamma = None\n",
    "    best_sigma = None\n",
    "    best_mse = np.inf\n",
    "\n",
    "    mean_mse_vals = np.zeros((len(gamma_values), len(sigma_values)))\n",
    "\n",
    "    for gamma_index, gamma in enumerate(gamma_values):\n",
    "        for sigma_index, sigma in enumerate(sigma_values):\n",
    "            mse_val_errors = []\n",
    "            \n",
    "            for i in range(5):\n",
    "                val_start = i * fold_size\n",
    "                val_end = (i + 1) * fold_size if i < 4 else n\n",
    "\n",
    "                # Validation set\n",
    "                X_val = X_train[val_start:val_end]\n",
    "                y_val = y_train[val_start:val_end]\n",
    "\n",
    "                # Training set\n",
    "                X_train_fold = np.concatenate([X_train[:val_start], X_train[val_end:]], axis=0)\n",
    "                y_train_fold = np.concatenate([y_train[:val_start], y_train[val_end:]], axis=0)\n",
    "\n",
    "                K_train = gaussian_kernel(X_train_fold, X_train_fold, sigma)\n",
    "                K_val = gaussian_kernel(X_val, X_train_fold, sigma)\n",
    "\n",
    "                # \"Training model\"\n",
    "                alpha_star = kernel_ridge_regression(X_train_fold, y_train_fold, gamma, K_train)\n",
    "\n",
    "                y_val_pred = K_val @ alpha_star\n",
    "\n",
    "                mse_val_error = np.mean((y_val - y_val_pred) ** 2)\n",
    "                mse_val_errors.append(mse_val_error)\n",
    "\n",
    "            mean_mse_vals[gamma_index, sigma_index] = np.mean(mse_val_errors)\n",
    "\n",
    "            if np.mean(mse_val_errors) < best_mse:\n",
    "                best_mse = np.mean(mse_val_errors)\n",
    "                best_gamma = gamma\n",
    "                best_sigma = sigma\n",
    "\n",
    "    return best_gamma, best_sigma, best_mse, mean_mse_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 20\n",
    "gamma_values = [2**(-40 + i) for i in range(15)]\n",
    "sigma_values = [2**(7 + i * 0.5) for i in range(13)]\n",
    "\n",
    "constant_train_errors = []\n",
    "constant_test_errors = []\n",
    "single_train_errors = np.zeros((num_runs, 12))\n",
    "single_test_errors = np.zeros((num_runs, 12))\n",
    "all_train_errors = []\n",
    "all_test_errors = []\n",
    "kernel_train_errors = []\n",
    "kernel_test_errors = []\n",
    "\n",
    "for run in range(num_runs):\n",
    "    X_train, y_train, X_test, y_test = split_dataset(random_state=run)\n",
    "\n",
    "    mse_train, mse_test = constant_attribute(X_train, y_train, X_test, y_test)\n",
    "    constant_train_errors.append(mse_train)\n",
    "    constant_test_errors.append(mse_test)\n",
    "\n",
    "    training_errors, test_errors = single_attribute(X_train, y_train, X_test, y_test)\n",
    "    single_train_errors[run] = training_errors\n",
    "    single_test_errors[run] = test_errors\n",
    "\n",
    "    mse_train, mse_test = all_attributes(X_train, y_train, X_test, y_test)\n",
    "    all_train_errors.append(mse_train)\n",
    "    all_test_errors.append(mse_test)\n",
    "\n",
    "\n",
    "    # Kernel Ridge Regression\n",
    "    best_gamma, best_sigma, best_mse, _ = cross_validation(X_train, y_train, gamma_values, sigma_values)\n",
    "\n",
    "    K_train_best = gaussian_kernel(X_train, X_train, best_sigma)\n",
    "    K_test_best = gaussian_kernel(X_test, X_train, best_sigma)\n",
    "\n",
    "    alpha_star_best = kernel_ridge_regression(X_train, y_train, best_gamma, K_train_best)\n",
    "\n",
    "    y_train_pred = K_train_best @ alpha_star_best\n",
    "    y_test_pred = K_test_best @ alpha_star_best\n",
    "\n",
    "    train_mse = np.mean((y_train - y_train_pred) ** 2)\n",
    "    test_mse = np.mean((y_test - y_test_pred) ** 2)\n",
    "\n",
    "    kernel_train_errors.append(train_mse)\n",
    "    kernel_test_errors.append(test_mse)\n",
    "\n",
    "con_mean_train = np.mean(constant_train_errors)\n",
    "con_mean_test = np.mean(constant_test_errors)\n",
    "con_std_train = np.std(constant_train_errors)\n",
    "con_std_test = np.std(constant_test_errors)\n",
    "\n",
    "sin_mean_train = np.mean(single_train_errors, axis=0)\n",
    "sin_mean_test = np.mean(single_test_errors, axis=0)\n",
    "sin_std_train = np.std(single_train_errors, axis=0)\n",
    "sin_std_test = np.std(single_test_errors, axis=0)\n",
    "\n",
    "all_mean_train = np.mean(all_train_errors)\n",
    "all_mean_test = np.mean(all_test_errors)\n",
    "all_std_train = np.std(all_train_errors)\n",
    "all_std_test = np.std(all_test_errors)\n",
    "\n",
    "ker_mean_train = np.mean(kernel_train_errors)\n",
    "ker_mean_test = np.mean(kernel_test_errors)\n",
    "ker_std_train = np.std(kernel_train_errors)\n",
    "ker_std_test = np.std(kernel_test_errors)\n",
    "\n",
    "# Take a couple of mins to run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Method     MSE train       MSE test\n",
      "0                     Naive Regression  84.54 ± 5.39  84.47 ± 10.76\n",
      "1      Linear Regression (attribute 1)  71.25 ± 4.89  73.91 ± 10.40\n",
      "2      Linear Regression (attribute 2)  74.19 ± 4.34   72.38 ± 8.77\n",
      "3      Linear Regression (attribute 3)  64.92 ± 4.35   64.56 ± 8.85\n",
      "4      Linear Regression (attribute 4)  82.02 ± 5.12  82.19 ± 10.62\n",
      "5      Linear Regression (attribute 5)  69.48 ± 4.45   68.48 ± 8.98\n",
      "6      Linear Regression (attribute 6)  43.36 ± 3.02   44.46 ± 5.91\n",
      "7      Linear Regression (attribute 7)  72.69 ± 4.81   72.26 ± 9.68\n",
      "8      Linear Regression (attribute 8)  79.44 ± 5.26  78.98 ± 10.54\n",
      "9      Linear Regression (attribute 9)  71.93 ± 4.88   73.08 ± 9.85\n",
      "10    Linear Regression (attribute 10)  65.59 ± 4.48   66.97 ± 9.02\n",
      "11    Linear Regression (attribute 11)  62.35 ± 3.99   63.83 ± 8.09\n",
      "12    Linear Regression (attribute 12)  38.70 ± 2.33   38.50 ± 4.63\n",
      "13  Linear Regression (all attributes)  22.28 ± 1.65   23.98 ± 3.59\n",
      "14             Kernel Ridge Regression   7.90 ± 1.61   12.24 ± 2.18\n"
     ]
    }
   ],
   "source": [
    "# Display table\n",
    "\n",
    "methods = [\n",
    "    \"Naive Regression\",\n",
    "    \"Linear Regression (attribute 1)\",\n",
    "    \"Linear Regression (attribute 2)\",\n",
    "    \"Linear Regression (attribute 3)\",\n",
    "    \"Linear Regression (attribute 4)\",\n",
    "    \"Linear Regression (attribute 5)\",\n",
    "    \"Linear Regression (attribute 6)\",\n",
    "    \"Linear Regression (attribute 7)\",\n",
    "    \"Linear Regression (attribute 8)\",\n",
    "    \"Linear Regression (attribute 9)\",\n",
    "    \"Linear Regression (attribute 10)\",\n",
    "    \"Linear Regression (attribute 11)\",\n",
    "    \"Linear Regression (attribute 12)\",\n",
    "    \"Linear Regression (all attributes)\",\n",
    "    \"Kernel Ridge Regression\"\n",
    "]\n",
    "\n",
    "data = {\n",
    "    'Method': methods,\n",
    "    'MSE train': [\n",
    "        f\"{con_mean_train:.2f} ± {con_std_train:.2f}\",\n",
    "        f\"{sin_mean_train[0]:.2f} ± {sin_std_train[0]:.2f}\",\n",
    "        f\"{sin_mean_train[1]:.2f} ± {sin_std_train[1]:.2f}\",\n",
    "        f\"{sin_mean_train[2]:.2f} ± {sin_std_train[2]:.2f}\",\n",
    "        f\"{sin_mean_train[3]:.2f} ± {sin_std_train[3]:.2f}\",\n",
    "        f\"{sin_mean_train[4]:.2f} ± {sin_std_train[4]:.2f}\",\n",
    "        f\"{sin_mean_train[5]:.2f} ± {sin_std_train[5]:.2f}\",\n",
    "        f\"{sin_mean_train[6]:.2f} ± {sin_std_train[6]:.2f}\",\n",
    "        f\"{sin_mean_train[7]:.2f} ± {sin_std_train[7]:.2f}\",\n",
    "        f\"{sin_mean_train[8]:.2f} ± {sin_std_train[8]:.2f}\",\n",
    "        f\"{sin_mean_train[9]:.2f} ± {sin_std_train[9]:.2f}\",\n",
    "        f\"{sin_mean_train[10]:.2f} ± {sin_std_train[10]:.2f}\",\n",
    "        f\"{sin_mean_train[11]:.2f} ± {sin_std_train[11]:.2f}\",\n",
    "        f\"{all_mean_train:.2f} ± {all_std_train:.2f}\",\n",
    "        f\"{ker_mean_train:.2f} ± {ker_std_train:.2f}\"\n",
    "    ],\n",
    "    'MSE test': [\n",
    "        f\"{con_mean_test:.2f} ± {con_std_test:.2f}\",\n",
    "        f\"{sin_mean_test[0]:.2f} ± {sin_std_test[0]:.2f}\",\n",
    "        f\"{sin_mean_test[1]:.2f} ± {sin_std_test[1]:.2f}\",\n",
    "        f\"{sin_mean_test[2]:.2f} ± {sin_std_test[2]:.2f}\",\n",
    "        f\"{sin_mean_test[3]:.2f} ± {sin_std_test[3]:.2f}\",\n",
    "        f\"{sin_mean_test[4]:.2f} ± {sin_std_test[4]:.2f}\",\n",
    "        f\"{sin_mean_test[5]:.2f} ± {sin_std_test[5]:.2f}\",\n",
    "        f\"{sin_mean_test[6]:.2f} ± {sin_std_test[6]:.2f}\",\n",
    "        f\"{sin_mean_test[7]:.2f} ± {sin_std_test[7]:.2f}\",\n",
    "        f\"{sin_mean_test[8]:.2f} ± {sin_std_test[8]:.2f}\",\n",
    "        f\"{sin_mean_test[9]:.2f} ± {sin_std_test[9]:.2f}\",\n",
    "        f\"{sin_mean_test[10]:.2f} ± {sin_std_test[10]:.2f}\",\n",
    "        f\"{sin_mean_test[11]:.2f} ± {sin_std_test[11]:.2f}\",\n",
    "        f\"{all_mean_test:.2f} ± {all_std_test:.2f}\",\n",
    "        f\"{ker_mean_test:.2f} ± {ker_std_test:.2f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
