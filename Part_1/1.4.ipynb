{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(filepath=\"./Boston-filtered.csv\", testsize=1/3, random_state=None):\n",
    "    \"\"\"\n",
    "    Split a dataset into training and test sets.\n",
    "\n",
    "    This function loads a dataset from a CSV file, shuffles the rows (if a random seed is provided), \n",
    "    and splits the dataset into input features (`X`) and target values (`y`). The dataset is then \n",
    "    divided into training and test sets based on the specified `testsize` ratio.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    filepath : str, optional\n",
    "        The path to the CSV file containing the dataset. The default is \"./Boston-filtered.csv\".\n",
    "    \n",
    "    testsize : float, optional\n",
    "        The proportion of the dataset to include in the test split. Default is 1/3, meaning \n",
    "        that 1/3 of the dataset will be used for testing and the rest for training.\n",
    "    \n",
    "    random_state : int, optional\n",
    "        The seed used by the random number generator for shuffling the data. If `None`, the data \n",
    "        will not be shuffled in a reproducible way. Default is `None`.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    X_train : ndarray\n",
    "        A numpy array containing the training input features.\n",
    "    \n",
    "    y_train : ndarray\n",
    "        A numpy array containing the training target values.\n",
    "    \n",
    "    X_test : ndarray\n",
    "        A numpy array containing the test input features.\n",
    "    \n",
    "    y_test : ndarray\n",
    "        A numpy array containing the test target values.\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    X_train, y_train, X_test, y_test = split_dataset(filepath=\"data.csv\", testsize=0.2, random_state=42)\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(filepath)\n",
    "\n",
    "    # Shuffle the dataset (if random_state is specified for reproducibility)\n",
    "    data = data.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    # Separate the features (X) and target values (y)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "\n",
    "    # Calculate the index to split the dataset based on the testsize ratio\n",
    "    split_index = int(len(data) * (1 - testsize))\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train = X[:split_index]\n",
    "    y_train = y[:split_index]\n",
    "\n",
    "    X_test = X[split_index:]\n",
    "    y_test = y[split_index:]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Naive Regression) MSE training: 84.54 ± 5.39, MSE test: 84.47 ± 10.76\n"
     ]
    }
   ],
   "source": [
    "# (a)\n",
    "num_runs = 20\n",
    "training_errors = []\n",
    "test_errors = []\n",
    "\n",
    "def constant_attribute(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Perform naive regression by predicting the mean of the training target values.\n",
    "    \n",
    "    This function predicts the target value using a constant model, where the prediction \n",
    "    for each sample is simply the mean of the target values in the training set. It then \n",
    "    calculates the Mean Squared Error (MSE) for both the training and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    X_train : ndarray\n",
    "        A 1D numpy array of training input features (not used in the constant model).\n",
    "    \n",
    "    y_train : ndarray\n",
    "        A 1D numpy array of training target values.\n",
    "    \n",
    "    X_test : ndarray\n",
    "        A 1D numpy array of test input features (not used in the constant model).\n",
    "    \n",
    "    y_test : ndarray\n",
    "        A 1D numpy array of test target values.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    mse_train : float\n",
    "        The Mean Squared Error (MSE) on the training set.\n",
    "    \n",
    "    mse_test : float\n",
    "        The Mean Squared Error (MSE) on the test set.\n",
    "    \n",
    "    \"\"\"\n",
    "    y_mean = np.mean(y_train)\n",
    "\n",
    "    X_train_ones = np.ones(len(X_train))\n",
    "    X_test_ones = np.ones(len(X_test))\n",
    "\n",
    "    y_train_pred = y_mean * X_train_ones\n",
    "    y_test_pred = y_mean * X_test_ones\n",
    "\n",
    "    mse_train = np.mean((y_train - y_train_pred)**2)\n",
    "    mse_test = np.mean((y_test - y_test_pred)**2)\n",
    "\n",
    "    return mse_train, mse_test\n",
    "\n",
    "for run in range(num_runs):\n",
    "    X_train, y_train, X_test, y_test = split_dataset(random_state=run)\n",
    "    constant_mse_train, constant_mse_test = constant_attribute(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    training_errors.append(constant_mse_train)\n",
    "    test_errors.append(constant_mse_test)\n",
    "\n",
    "mean_training_error = np.mean(training_errors)\n",
    "std_training_error = np.std(training_errors)\n",
    "mean_test_error = np.mean(test_errors)\n",
    "std_test_error = np.std(test_errors)\n",
    "\n",
    "print(f\"(Naive Regression) MSE training: {mean_training_error:.2f} ± {std_training_error:.2f}, MSE test: {mean_test_error:.2f} ± {std_test_error:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Although the constant prediction model provides a basic, computationally simple solution, its inability to account for feature-target relationships leads to high MSE values. This highlights the limitations of using a naive approach in predictive modeling, as it fails to capture the complexity of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression (attribute 1) - MSE train: 71.25 ± 4.89, MSE test: 73.91 ± 10.40\n",
      "Linear Regression (attribute 2) - MSE train: 74.19 ± 4.34, MSE test: 72.38 ± 8.77\n",
      "Linear Regression (attribute 3) - MSE train: 64.92 ± 4.35, MSE test: 64.56 ± 8.85\n",
      "Linear Regression (attribute 4) - MSE train: 82.02 ± 5.12, MSE test: 82.19 ± 10.62\n",
      "Linear Regression (attribute 5) - MSE train: 69.48 ± 4.45, MSE test: 68.48 ± 8.98\n",
      "Linear Regression (attribute 6) - MSE train: 43.36 ± 3.02, MSE test: 44.46 ± 5.91\n",
      "Linear Regression (attribute 7) - MSE train: 72.69 ± 4.81, MSE test: 72.26 ± 9.68\n",
      "Linear Regression (attribute 8) - MSE train: 79.44 ± 5.26, MSE test: 78.98 ± 10.54\n",
      "Linear Regression (attribute 9) - MSE train: 71.93 ± 4.88, MSE test: 73.08 ± 9.85\n",
      "Linear Regression (attribute 10) - MSE train: 65.59 ± 4.48, MSE test: 66.97 ± 9.02\n",
      "Linear Regression (attribute 11) - MSE train: 62.35 ± 3.99, MSE test: 63.83 ± 8.09\n",
      "Linear Regression (attribute 12) - MSE train: 38.70 ± 2.33, MSE test: 38.50 ± 4.63\n"
     ]
    }
   ],
   "source": [
    "# (c)\n",
    "def single_attribute(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Perform linear regression on each attribute (feature) of the training and test datasets, \n",
    "    using a single attribute at a time, augmented with a column of ones (bias term).\n",
    "    \n",
    "    Parameters:\n",
    "    X_train (numpy.ndarray): The training feature matrix (n_samples x n_features).\n",
    "    y_train (numpy.ndarray): The training target vector (n_samples,).\n",
    "    X_test (numpy.ndarray): The test feature matrix (n_samples x n_features).\n",
    "    y_test (numpy.ndarray): The test target vector (n_samples,).\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing two lists:\n",
    "        - training_errors (list): A list of Mean Squared Errors (MSE) for each attribute in the training set.\n",
    "        - test_errors (list): A list of MSEs for each attribute in the test set.\n",
    "    \"\"\"\n",
    "    training_errors = [] \n",
    "    test_errors = [] \n",
    "\n",
    "    for i in range(X_train.shape[1]):\n",
    "        X_train_single = X_train[:, i].reshape(-1, 1)\n",
    "        X_test_single = X_test[:, i].reshape(-1, 1)\n",
    "\n",
    "        X_train_augmented = np.hstack([X_train_single, np.ones((X_train_single.shape[0], 1))])\n",
    "        X_test_augmented = np.hstack([X_test_single, np.ones((X_test_single.shape[0], 1))])\n",
    "\n",
    "        w = np.linalg.inv(X_train_augmented.T @ X_train_augmented) @ X_train_augmented.T @ y_train\n",
    "\n",
    "        y_train_pred = X_train_augmented @ w\n",
    "        y_test_pred = X_test_augmented @ w\n",
    "\n",
    "        mse_train = np.mean((y_train - y_train_pred)**2)\n",
    "        mse_test = np.mean((y_test - y_test_pred)**2)\n",
    "\n",
    "        training_errors.append(mse_train)\n",
    "        test_errors.append(mse_test)    \n",
    "\n",
    "    return training_errors, test_errors\n",
    "\n",
    "num_runs = 20\n",
    "all_mse_train = np.zeros((num_runs, 12))\n",
    "all_mse_test = np.zeros((num_runs, 12))\n",
    "\n",
    "for run in range(num_runs):\n",
    "    X_train, y_train, X_test, y_test = split_dataset(random_state=run)\n",
    "\n",
    "    mse_train, mse_test = single_attribute(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    all_mse_train[run] = mse_train\n",
    "    all_mse_test[run] = mse_test\n",
    "\n",
    "mean_mse_single_train = np.mean(all_mse_train, axis=0)\n",
    "std_mse_single_train = np.std(all_mse_train, axis=0)\n",
    "mean_mse_single_test = np.mean(all_mse_test, axis=0)\n",
    "std_mse_single_test = np.std(all_mse_test, axis=0)\n",
    "\n",
    "for i in range(12):\n",
    "    print(f'Linear Regression (attribute {i+1}) - MSE train: {mean_mse_single_train[i]:.2f} ± {std_mse_single_train[i]:.2f}, MSE test: {mean_mse_single_test[i]:.2f} ± {std_mse_single_test[i]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression (all attributes) MSE training: 22.28 ± 1.65, MSE test: 23.98 ± 3.59\n"
     ]
    }
   ],
   "source": [
    "# (d)\n",
    "def all_attributes(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Perform linear regression using all features in the training and test datasets.\n",
    "    This function augments the feature matrix with a column of ones (for the bias term),\n",
    "    solves for the weights using the Normal Equation, and computes the MSE for both the training and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train (numpy.ndarray): The training feature matrix (n_samples x n_features).\n",
    "    y_train (numpy.ndarray): The training target vector (n_samples,).\n",
    "    X_test (numpy.ndarray): The test feature matrix (n_samples x n_features).\n",
    "    y_test (numpy.ndarray): The test target vector (n_samples,).\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing two values:\n",
    "        - mse_train (float): The Mean Squared Error for the training set.\n",
    "        - mse_test (float): The Mean Squared Error for the test set.\n",
    "    \"\"\"\n",
    "    X_train_augmented = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_test_augmented = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "\n",
    "    w = np.linalg.inv(X_train_augmented.T @ X_train_augmented) @ X_train_augmented.T @ y_train\n",
    "\n",
    "    y_train_pred = X_train_augmented @ w\n",
    "    y_test_pred = X_test_augmented @ w\n",
    "\n",
    "    mse_train = np.mean((y_train - y_train_pred)**2)\n",
    "    mse_test = np.mean((y_test - y_test_pred)**2)\n",
    "\n",
    "    return mse_train, mse_test\n",
    "\n",
    "num_runs = 20\n",
    "all_mse_train = np.zeros(num_runs)\n",
    "all_mse_test = np.zeros(num_runs)\n",
    "\n",
    "for run in range(num_runs):\n",
    "    X_train, y_train, X_test, y_test = split_dataset(random_state=run)\n",
    "    mse_train, mse_test = all_attributes(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    all_mse_train[run] = mse_train\n",
    "    all_mse_test[run] = mse_test\n",
    "\n",
    "mean_mse_train = np.mean(all_mse_train)\n",
    "std_mse_train = np.std(all_mse_train)\n",
    "mean_mse_test = np.mean(all_mse_test)\n",
    "std_mse_test = np.std(all_mse_test)\n",
    "\n",
    "print(f\"Linear Regression (all attributes) MSE training: {mean_mse_train:.2f} ± {std_mse_train:.2f}, MSE test: {mean_mse_test:.2f} ± {std_mse_test:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
